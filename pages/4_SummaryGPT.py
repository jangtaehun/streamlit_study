import streamlit as st
from openai import OpenAI
import glob
import subprocess
from pydub import AudioSegment
import math
import os
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import StrOutputParser

# Q&A
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings
from langchain.storage import LocalFileStore


splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=800,
    chunk_overlap=100,
)


# QnA
def embed_file(file_path, file_name):
    # file_content = file.read()
    # file_path = f"files/{file.name}"
    # with open(file_path, "wb") as f:
    #     f.write(file_content)
    # ì´ë¯¸ íŒŒì¼ì´ ìˆê¸° ë•Œë¬¸ì— ìƒëµì´ ê°€ëŠ¥í•˜ë‹¤.

    cache_dir = LocalFileStore(f"files/embeddings/{file_name}")
    loader = TextLoader(file_path)
    docs = loader.load_and_split(text_splitter=splitter)
    embeddings = OpenAIEmbeddings()
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)
    vectorstore = FAISS.from_documents(docs, cached_embeddings)
    retriever = vectorstore.as_retriever()
    return retriever


####

client = OpenAI()


def transcribe_chunks(chunk_folder, destination):
    # destination: text file ì €ì¥ë  ìœ„ì¹˜
    # globì€ ìˆœì„œëŒ€ë¡œ ê°€ì ¸ì˜¤ì§€ ì•ŠëŠ”ë‹¤. -> ì •ë ¬ í•„ìš”
    files = glob.glob(f"{chunk_folder}/*.mp3")
    files.sort()
    for file in files:
        with open(file, "rb") as audio_file, open(
            destination, "a", encoding="UTF-8"
        ) as text_file:
            transcript = client.audio.transcriptions.create(
                model="whisper-1",
                file=audio_file,
            )
            print(transcript)
            text_file.write(transcript.text)


def extract_audio_from_video(video_path):
    audio_path = video_path.replace("mp4", "mp3")
    command = [
        "ffmpeg",
        "-y",  # í•­ìƒ overwrite(ë®ì–´ì“°ê¸°)
        "-i",
        video_path,
        "-vn",
        audio_path,
    ]
    subprocess.run(command)


def cut_audio_in_chunks(audio_path, chunk_size, chunks_folder, name):
    track = AudioSegment.from_mp3(audio_path)
    chunk_len = chunk_size * 60 * 1000  # ì²« ì‹œê°„ ì„ íƒ, *1000: pydubì€ ë°€ë¦¬ì´ˆë‹¨ìœ„ë¡œ ì‘ë™
    chunks = math.ceil(len(track) / chunk_len)

    for i in range(chunks):
        start_time = i * chunk_len
        end_time = (i + 1) * chunk_len

        chunk = track[start_time:end_time]

        chunk.export(f"{chunks_folder}/{name}_{i}.mp3", format="mp3")


def start_video():
    chunks_folder = "files/chunks"
    with st.status("Loading video...") as status:
        video_content = video.read()
        video_path = f"files/{video.name}"
        audio_path = video_path.replace("mp4", "mp3")
        transcript_path = video_path.replace("mp4", "txt")
        with open(video_path, "wb") as f:
            f.write(video_content)

        status.update(label="Extracting audio...")
        # ë¹„ë””ì˜¤ -> mp3ë¡œ ë³€í™˜
        extract_audio_from_video(video_path)

        status.update(label="Cutting audio segments...")
        # mp3ë¥¼ ì˜ê²Œ ë‚˜ëˆ ì„œ ì €ì¥
        cut_audio_in_chunks(audio_path, 10, chunks_folder, f"{video.name}")

        status.update(label="Transcribing audio...")
        # mp3ì—ì„œ ì¶•ì¶œí•´ txt íŒŒì¼ë¡œ ì €ì¥
        transcribe_chunks(chunks_folder, transcript_path)


def show_tab(video_name):
    # ê²½ë¡œ ì§€ì •
    video_path = f"files/{video_name}"
    # audio_path = video_path.replace("mp4", "mp3")
    transcript_path = video_path.replace("mp4", "txt")

    transcribe_tab, summary_tab, qna_tab = st.tabs(["Transcript", "Summary", "Q&A"])
    with transcribe_tab:
        with open(transcript_path, "r") as file:
            st.write(file.read())
    with summary_tab:
        start = st.button("Generate summary")

        if start:
            # ë‘ ê°œì˜ chainì„ ë§Œë“ ë‹¤.
            # 1. ìš”ì•½
            # 2. ë‹¤ë¥¸ ëª¨ë“  documentë¥¼ ìš”ì•½ (ì´ì „ì˜ ìš”ì•½ê³¼ ìƒˆ contextë¥¼ ì‚¬ìš©í•´ ìƒˆë¡œìš´ ìš”ì•½ ë§Œë“¤ê¸°)
            file_name = video.name.replace("mp4", "txt")
            loader = TextLoader(f"files/{file_name}")

            docs = loader.load_and_split(text_splitter=splitter)
            st.write(docs)

            first_summary_prompt = ChatPromptTemplate.from_template(
                """
                ë‹¤ìŒ ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”.
                "{text}"
                ê°„ê²°í•œ ìš”ì•½:
            """
            )
            first_summary_chain = first_summary_prompt | llm | StrOutputParser()
            summary = first_summary_chain.invoke({"text": docs[0].page_content})
            # summary = first_summary_chain.invoke({"text": docs[0].page_content}).content -> StrOutputParser()ì´ ëª¨ë“  ê²ƒì„ stringìœ¼ë¡œ ë³€í™˜

            # test_summary_chain = first_summary_prompt | llm
            # test_summary = test_summary_chain.invoke({"text": docs[0].page_content})
            # st.write(test_summary.content)
            # st.write(test_summary) -> -> content='ë‹¤ì–‘í•œ ìŒì‹ê³¼ ì œí’ˆ ì†Œê°œë¥¼ í†µí•´ ìì‚°ê´€ë¦¬ì™€ ê±´ê°•ì— ëŒ€í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ê´‘ê³ ì´ë‹¤.'

            refine_prompt = ChatPromptTemplate.from_template(
                """
                ë‹¹ì‹ ì˜ ì—­í• ì€ ë§ˆì§€ë§‰ ìš”ì•½ë³¸ì„ ìƒì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
                ìš°ë¦¬ëŠ” ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ìš”ì•½ë³¸ì„ ì œê³µí•  ê²ƒì…ë‹ˆë‹¤.
                ìš”ì•½ë³¸: {existing_summary}
                ìš°ë¦¬ëŠ” í•„ìš”í•œ ê²½ìš°ì—ë§Œ ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ìš”ì•½ë³¸ì„ ê°€ë‹¤ë“¬ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                -----------------
                {context}
                -----------------
                ì£¼ì–´ì§„ ìƒˆë¡œìš´ textë¥¼ í†µí•´, ê¸°ì¡´ì˜ ìš”ì•½ë³¸ì„ ê°€ë‹¤ë“¬ìœ¼ì„¸ìš”.
                ë§Œì•½ contextê°€ ìœ ìš©í•˜ì§€ ì•Šë‹¤ë©´, ê¸°ì¡´ì˜ ìš”ì•½ë³¸ì„ return í•˜ì„¸ìš”.
                """
            )
            refine_chain = refine_prompt | llm | StrOutputParser()

            with st.status("Summarizing...") as status:
                for i, doc in enumerate(docs[:]):
                    status.update(label=f"Processing document {i}/{len(docs)-1}")
                    summary = refine_chain.invoke(
                        {"existing_summary": summary, "context": doc.page_content}
                    )
                    st.write(summary)
            st.write(summary)

    with qna_tab:
        retriever = embed_file(transcript_path, video_name)
        docs = retriever.invoke("ë°ì´í„°ì— ëŒ€í•´ ì–´ë–¤ ë‚´ìš©ì„ ì´ì•¼ê¸°í•˜ê³  ìˆë‚˜ìš”?")
        st.write(docs)


######
st.set_page_config(
    page_title="SummaryGPT",
    page_icon="ğŸ’½",
)

# streamlits
with st.sidebar:
    openaikey = None
    openaikey = st.text_input("Write Your OpenAI API key: ", type="password")
    os.environ["OPENAI_API_KEY"] = openaikey

st.markdown(
    """
        # SummaryGPT
    """
)

if openaikey:
    llm = ChatOpenAI(
        temperature=0.1,
    )

    video = st.file_uploader("video", type=["mp4", "avi", "mkv", "mov"])

    if video:
        has_transcript = os.path.exists(f"files/{video.name}")
        if has_transcript:
            show_tab(video.name)
        else:
            start_video()
            show_tab(video.name)
else:
    st.markdown(
        """
            Welcome to SummaryGPT, upload a video and i will give you a transcript, a summary and a chat bot to ask any questions about it.
            
            Start by writing OpenAI API key and uploading a video file in the sidebar.
            """
    )
